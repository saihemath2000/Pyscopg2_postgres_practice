{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c292f41-bf29-4075-b711-2841724a5061",
   "metadata": {},
   "source": [
    "## Copying data from position_geography for a particular team_instance_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab30a5-927e-4eca-a553-717a65287bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src ---> position_geography   dest----> hg_position_geo\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcae034d-5e07-419a-8139-3e58bc4f5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"next_gen\"\n",
    "DB_USER = \"sde\"\n",
    "DB_PASS = \"sde\"\n",
    "DB_HOST = \"salesiqgen2.cygagau4oro0.us-west-2.rds.amazonaws.com\"\n",
    "DB_PORT = \"5432\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a545a43-2e4d-47bb-80f9-90add4582994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection():\n",
    "    # Establish the connection to the database\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, \n",
    "            user=DB_USER, \n",
    "            password=DB_PASS, \n",
    "            host=DB_HOST, \n",
    "            port=DB_PORT\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to connect to the database: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b504f9-4ee9-4fcf-ae8c-6c3c6fde8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d94513-21b6-4988-99f9-c18629f2b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_team_instance_id(conn, team_instance_id):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            query = \"SELECT EXISTS (SELECT 1 FROM position_geography WHERE team_instance_id = %s)\"\n",
    "            cur.execute(query, (team_instance_id,))\n",
    "            return cur.fetchone()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking team_instance_id: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d8dc5-b5f4-4a21-a756-848c3bbab5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rows(conn, team_instance_id):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            fetch_query = \"SELECT * FROM position_geography WHERE team_instance_id = %s\"\n",
    "            cur.execute(fetch_query, (team_instance_id,))\n",
    "            return cur.fetchall()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching rows: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c81427-ddfd-4d1f-a6d0-64c2b40fd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c759af6-3313-4bf6-9c95-fb38140d11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a1h8W00000Ok8KXQAZ\n",
    "team_instance_id = input(\"Enter team_instance_id: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599083c-44dc-41e2-9139-8bcc2900e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_rows(conn, rows):\n",
    "    try:\n",
    "        # Convert fetched rows to DataFrame\n",
    "        columns=[\"id\",\"zip\",\"name\",\"zip_type\",\"state\",\"center_x\",\"center_y\",\"x_min\",\"x_max\",\"y_min\",\"y_max\"\n",
    "                ,\"related_zip\",\"shape\",\"doughnut_zips\",\"neighbour_zips\",\"level1_code\",\"level1_name\",\"level2_code\"\n",
    "                ,\"level2_name\",\"level3_code\",\"level3_name\",\"level4_code\",\"level4_name\",\"level5_code\",\"level5_name\"\n",
    "                ,\"level6_code\",\"level6_name\",\"level7_code\",\"level7_name\",\"level8_code\",\"level8_name\",\"level9_code\"\n",
    "                ,\"level9_name\",\"level10_code\",\"level10_name\",\"team_name\",\"team_instance_code\",\"team_type\"\n",
    "                ,\"boundary_refresh_flag\",\"level1_rgb\",\"level2_rgb\",\"level3_rgb\",\"level4_rgb\",\"level5_rgb\",\"st_areas\"\n",
    "                ,\"st_lengths\",\"proposed_position_code\",\"previous_position_code\",\"status\",\"metric_data\",\"team_instance_id\"\n",
    "                ,\"salesforce_org_id\",\"salesforce_record_id\",\"origin\",\"event_id\",\"county_name\",\"account_metric_data\"\n",
    "                ,\"account_count\",\"geography_zip_type\",\"nearest_neighbor\",\"is_simulation\",\"original_metric_data\"]\n",
    "       \n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "       # print(df.dtypes)\n",
    "        \n",
    "        # Convert DataFrame to list of tuples with native Python types\n",
    "        def convert_value(value):\n",
    "            if isinstance(value, (np.int64, np.float64, np.bool_)):\n",
    "                return value.item()\n",
    "            return value\n",
    "        \n",
    "        records_list = [tuple(map(convert_value, row)) for row in df.to_records(index=False)]\n",
    "        \n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO hg_position_geo ({\", \".join(columns)})\n",
    "            VALUES ({\", \".join([\"%s\"] * len(columns))})\n",
    "        \"\"\"\n",
    "        \n",
    "        # Bulk insert using execute_batch\n",
    "        with conn.cursor() as cur:\n",
    "            execute_batch(cur, insert_query, records_list)\n",
    "        conn.commit()\n",
    "        print(f\"Rows copied to hg_position_geo successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error copying rows: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94dba4-2c34-4c2c-a44f-58659a3776c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_team_instance_id(conn, team_instance_id):\n",
    "    rows = fetch_rows(conn, team_instance_id)\n",
    "    if rows:\n",
    "        copy_rows(conn, rows)\n",
    "    else:\n",
    "        print(f\"No rows found for team_instance_id {team_instance_id}.\")\n",
    "else:\n",
    "    print(f\"team_instance_id {team_instance_id} does not exist in position_geography.\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fa5f2-6f0b-41c0-9bc9-658b675801e2",
   "metadata": {},
   "source": [
    "## Populating data from {hg_position_geo} to {hg_positions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714c2304-602e-4471-8b48-16b08f575443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from psycopg2.extras import execute_batch\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b34507e-cfdc-4529-a38e-615877f73364",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fe4736-0f23-406d-9a43-96117e3f6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src=\"hg_position_geo\"\n",
    "dest=\"hg_positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe3a2ef-82a6-4be7-837d-9f24412e1b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection pool created successfully\n",
      "Script completed in 59.66 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from psycopg2.extras import execute_batch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "DB_NAME = \"next_gen\"\n",
    "DB_USER = \"sde\"\n",
    "DB_PASS = \"sde\"\n",
    "DB_HOST = \"salesiqgen2.cygagau4oro0.us-west-2.rds.amazonaws.com\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Create a connection pool\n",
    "try:\n",
    "    connection_pool = psycopg2.pool.ThreadedConnectionPool(1, 10, database=DB_NAME, user=DB_USER, password=DB_PASS, host=DB_HOST, port=DB_PORT)\n",
    "    if connection_pool:\n",
    "        print(\"Connection pool created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection pool creation failed: {e}\")\n",
    "\n",
    "# Function to find max level\n",
    "def find_max_level(cur):\n",
    "    max_level = 0\n",
    "    level = 1\n",
    "    while True:\n",
    "        code_column = f\"level{level}_code\"\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {src} WHERE {code_column} IS NOT NULL\")\n",
    "        count = cur.fetchone()[0]\n",
    "        if count == 0:\n",
    "            break\n",
    "        max_level = level\n",
    "        level += 1\n",
    "    return max_level\n",
    "\n",
    "# Main processing\n",
    "try:\n",
    "    # Get a connection from the pool\n",
    "    conn = connection_pool.getconn()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Find the maximum level\n",
    "    max_level = find_max_level(cur)\n",
    "    \n",
    "    # Get the metric keys\n",
    "    temp_query = f\"\"\"SELECT DISTINCT jsonb_object_keys(CAST(metric_data AS jsonb)) AS metric_key FROM {src}\"\"\"\n",
    "    cur.execute(temp_query)\n",
    "    metric_keys = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    # Prepare metric data items for the SQL query\n",
    "    metric_data_items = ', '.join([f\"'{metric_key}', SUM(COALESCE((metric_data::jsonb->>'{metric_key}')::float, 0))\" for metric_key in metric_keys])\n",
    "\n",
    "    # Function to execute a single query\n",
    "    def execute_query(query):\n",
    "        conn = connection_pool.getconn()\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "                return cur.fetchall()\n",
    "        finally:\n",
    "            connection_pool.putconn(conn)\n",
    "\n",
    "    queries = []\n",
    "    for level in range(1, max_level + 1):\n",
    "        code_column = f\"level{level}_code\"\n",
    "        code_name = f\"level{level}_name\"\n",
    "        queries.append(f\"\"\"\n",
    "            SELECT \n",
    "                {code_name} AS name, \n",
    "                jsonb_build_object(\n",
    "                    {metric_data_items}\n",
    "                ) AS metric_data,\n",
    "                ST_Union(shape) AS shape,\n",
    "                {level} AS hierarchy_level,\n",
    "                (SELECT level{level+1}_code FROM {src} AS sub_src WHERE sub_src.{code_column} = {src}.{code_column}\n",
    "                limit 1) AS parent_position_code,\n",
    "                {code_column} AS client_position_code,\n",
    "                {code_column} AS client_territory_code,\n",
    "                MAX(salesforce_org_id) AS salesforce_org_id,\n",
    "                MAX(salesforce_record_id) AS salesforce_record_id\n",
    "            FROM {src}\n",
    "            GROUP BY {code_column}, {code_name}\n",
    "        \"\"\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(execute_query, queries))\n",
    "\n",
    "    # Flatten the results\n",
    "    results = [item for sublist in results for item in sublist]\n",
    "\n",
    "    # Define the columns for the DataFrame\n",
    "    columns = [\"name\", \"metric_data\", \"shape\",\"hierarchy_level\",\"parent_position_code\",\"client_position_code\",\"client_territory_code\",\"salesforce_org_id\", \"salesforce_record_id\"]\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    # Convert metric_data to JSON string\n",
    "    df[\"metric_data\"] = df[\"metric_data\"].apply(lambda x: json.dumps(x))\n",
    "\n",
    "    # Convert DataFrame to list of tuples with native Python types\n",
    "    def convert_value(value):\n",
    "        if isinstance(value, (np.int64, np.float64, np.bool_)):\n",
    "            return value.item()\n",
    "        return value\n",
    "\n",
    "    records_list = [tuple(map(convert_value, row)) for row in df.to_records(index=False)]\n",
    "\n",
    "    # Define the insert query\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO {dest} ({\", \".join(df.columns)})\n",
    "        VALUES ({\", \".join([\"%s\"] * len(df.columns))})\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the bulk insert\n",
    "    execute_batch(cur, insert_query, records_list)\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and return the connection to the pool\n",
    "    cur.close()\n",
    "    connection_pool.putconn(conn)\n",
    "\n",
    "    # Close all connections in the pool\n",
    "    connection_pool.closeall()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Script completed in {elapsed_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
